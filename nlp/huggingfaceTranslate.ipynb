{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WMT21 Translation\n",
    "\n",
    "https://huggingface.co/facebook/wmt21-dense-24-wide-x-en\n",
    "\n",
    "WMT 21 X-En is a 4.7B multilingual encoder-decoder (seq-to-seq) model trained for one-to-many multilingual translation. It was introduced in this paper and first released in this repository.\n",
    "\n",
    "The model can directly translate text from 7 languages: Hausa (ha), Icelandic (is), Japanese (ja), Czech (cs), Russian (ru), Chinese (zh), German (de) to English.\n",
    "\n",
    "To translate into a target language, the target language id is forced as the first generated token. To force the target language id as the first generated token, pass the forced_bos_token_id parameter to the generate method.\n",
    "\n",
    "Since the model was trained with domain tags, you should prepend them to the input as well.\n",
    "\n",
    "\"wmtdata newsdomain\": Use for sentences in the news domain\n",
    "\"wmtdata otherdomain\": Use for sentences in all other domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaikailiu/miniconda3/envs/mypy310/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "#https://huggingface.co/facebook/wmt21-dense-24-wide-en-x\n",
    "model_name=\"facebook/wmt21-dense-24-wide-en-x\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, tokenizer, textinput):\n",
    "    newsdomain=\"wmtdata newsdomain \"\n",
    "    otherdomain=\"wmtdata otherdomain \"\n",
    "    textinput = newsdomain+textinput\n",
    "    inputs = tokenizer(textinput, return_tensors=\"pt\")\n",
    "\n",
    "    # translate English to Chinese\n",
    "    generated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.get_lang_id(\"zh\")) #max_new_tokens\n",
    "    result=tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaikailiu/miniconda3/envs/mypy310/lib/python3.10/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (200) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['以色列军队搜查加沙的希法医院,寻找哈马斯存在的证据。']\n"
     ]
    }
   ],
   "source": [
    "textinput=\"Israeli troops scour Gaza’s al-Shifa Hospital for evidence of Hamas.\"\n",
    "result = translate(model, tokenizer, textinput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['以色列对加沙地带最大医院的突袭行动于周四进入第二天,以军搜查了哈马斯在该医院地下拥有大量基础设施的证据,以色列和美国官员称这些基础设施位于该设施之下。以色列国防军周四表示,搜查行动在医院附近的一所房屋中发现了一名被俘以色列妇女的尸体,以及武器。周三,以色列国防军发布了据称属于哈马斯的小型武器藏匿处的照片和视频。军方周四在其案例中添加了一张照片和一段视频,该照片和视频显示了一个粗糙的空洞,该空洞被描述为\"操作隧道竖井\"。《华盛顿邮报》核实了该竖井在al-Shifa医院建筑群内的位置,但无法核实该开口通向何处,也无法核实其目的。']\n"
     ]
    }
   ],
   "source": [
    "textinput=\"The Israeli raid of the Gaza Strip’s largest hospital stretched into its second day Thursday as troops searched for evidence of the extensive Hamas infrastructure that Israeli and U.S. officials have said lies beneath the facility. The Israel Defense Forces said Thursday that searches had uncovered the body of a captive Israeli woman in a house near the hospital, along with weapons. On Wednesday, the IDF released photographs and video of small caches of weapons it said belonged to Hamas. The military added to its case Thursday with a photo and video of a rough cavity that it described as an “operational tunnel shaft.” The Washington Post verified the location of the shaft inside the al-Shifa Hospital complex but could not verify where the opening led or what its purpose might be.\"\n",
    "result = translate(model, tokenizer, textinput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M2M100ForConditionalGeneration(\n",
       "  (model): M2M100Model(\n",
       "    (shared): Embedding(128009, 2048, padding_idx=1)\n",
       "    (encoder): M2M100Encoder(\n",
       "      (embed_tokens): Embedding(128009, 2048, padding_idx=1)\n",
       "      (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x M2M100EncoderLayer(\n",
       "          (self_attn): M2M100Attention(\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "          (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): M2M100Decoder(\n",
       "      (embed_tokens): Embedding(128009, 2048, padding_idx=1)\n",
       "      (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x M2M100DecoderLayer(\n",
       "          (self_attn): M2M100Attention(\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): M2M100Attention(\n",
       "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "            (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=2048, out_features=16384, bias=True)\n",
       "          (fc2): Linear(in_features=16384, out_features=2048, bias=True)\n",
       "          (final_layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128009, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device='mps'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, tokenizer, textinput, device):\n",
    "    newsdomain=\"wmtdata newsdomain \"\n",
    "    otherdomain=\"wmtdata otherdomain \"\n",
    "    textinput = newsdomain+textinput\n",
    "    inputs = tokenizer(textinput, return_tensors=\"pt\")\n",
    "    inputs.to(device)\n",
    "\n",
    "    # translate English to Chinese\n",
    "    generated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.get_lang_id(\"zh\")) #max_new_tokens\n",
    "    result=tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaikailiu/miniconda3/envs/mypy310/lib/python3.10/site-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (200) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "/Users/kaikailiu/miniconda3/envs/mypy310/lib/python3.10/site-packages/transformers/generation/utils.py:690: UserWarning: MPS: no support for int64 repeats mask, casting it to int32 (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1682343686130/work/aten/src/ATen/native/mps/operations/Repeat.mm:236.)\n",
      "  input_ids = input_ids.repeat_interleave(expand_size, dim=0)\n",
      "/Users/kaikailiu/miniconda3/envs/mypy310/lib/python3.10/site-packages/transformers/generation/beam_search.py:358: UserWarning: MPS: no support for int64 min/max ops, casting it to int32 (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1682343686130/work/aten/src/ATen/native/mps/operations/ReduceOps.mm:1271.)\n",
      "  sent_lengths_max = sent_lengths.max().item() + 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['以色列对加沙地带最大医院的突袭行动于周四进入第二天,以军搜查了哈马斯在该医院地下拥有大量基础设施的证据,以色列和美国官员称这些基础设施位于该设施之下。以色列国防军周四表示,搜查行动在医院附近的一所房屋中发现了一名被俘以色列妇女的尸体,以及武器。周三,以色列国防军发布了据称属于哈马斯的小型武器藏匿处的照片和视频。军方周四在其案例中添加了一张照片和一段视频,该照片和视频显示了一个粗糙的空洞,该空洞被描述为\"操作隧道竖井\"。《华盛顿邮报》核实了该竖井在al-Shifa医院建筑群内的位置,但无法核实该开口通向何处,也无法核实其目的。']\n"
     ]
    }
   ],
   "source": [
    "textinput=\"The Israeli raid of the Gaza Strip’s largest hospital stretched into its second day Thursday as troops searched for evidence of the extensive Hamas infrastructure that Israeli and U.S. officials have said lies beneath the facility. The Israel Defense Forces said Thursday that searches had uncovered the body of a captive Israeli woman in a house near the hospital, along with weapons. On Wednesday, the IDF released photographs and video of small caches of weapons it said belonged to Hamas. The military added to its case Thursday with a photo and video of a rough cavity that it described as an “operational tunnel shaft.” The Washington Post verified the location of the shaft inside the al-Shifa Hospital complex but could not verify where the opening led or what its purpose might be.\"\n",
    "result = translate(model, tokenizer, textinput, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypy310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
