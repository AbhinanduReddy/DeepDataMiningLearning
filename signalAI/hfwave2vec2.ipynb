{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#hfhome_dir=\"/data/cmpe249-fa23/Huggingfacecache\"\n",
    "hfhome_dir=os.path.join('D:',os.sep, 'Cache','huggingface')\n",
    "#os.environ['TRANSFORMERS_CACHE'] = hfhome_dir\n",
    "os.environ['HF_HOME'] = hfhome_dir\n",
    "#os.environ['HF_HUB_CACHE'] = os.path.join(hfhome_dir, 'hub')\n",
    "#os.environ['HF_DATASETS_CACHE'] = hfhome_dir\n",
    "#HF_HUB_OFFLINE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "dataset_split = load_dataset(\n",
    "        \"librispeech_asr\", #'librispeech_asr'\n",
    "        'clean',\n",
    "        split='train.100',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "    num_rows: 28539\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "raw_datasets = DatasetDict()\n",
    "raw_datasets[\"train\"] = dataset_split\n",
    "validation_split_percentage = 1\n",
    "num_validation_samples = raw_datasets[\"train\"].num_rows * validation_split_percentage // 100\n",
    "\n",
    "raw_datasets[\"validation\"] = raw_datasets[\"train\"].select(range(num_validation_samples))\n",
    "raw_datasets[\"train\"] = raw_datasets[\"train\"].select(range(num_validation_samples, raw_datasets[\"train\"].num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "        num_rows: 28254\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
       "        num_rows: 285\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = raw_datasets.remove_columns([\"speaker_id\", \"chapter_id\", \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['file', 'audio', 'text'],\n",
       "        num_rows: 28254\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['file', 'audio', 'text'],\n",
       "        num_rows: 285\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IN THIS HIS OPENING EFFORT TO REGAIN POSSESSION OF CORA DO MY YOUNG MEN LEAVE THE DELAWARES ROOM ON THE MOUNTAINS FOR THEIR HUNTS HE AT LENGTH CONTINUED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IS THERE ANYTHING THE LEAST WONDERFUL OR REMARKABLE ABOUT THIS NEIGHBOURHOOD I SHOULD JUST THINK THERE WAS REPLIED THE OTHER MANY THINGS THAT DON'T EXIST ANYWHERE ELSE IN THE WORLD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WHEN THEY ARRIVED THERE THEY MARCHED STRAIGHT TO ROME AND AFTER SOME FIGHTING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AT THE BOTTOM OF THE STREET HOWEVER SHE LOOKED BACK AGAIN AND THEN NOT AT A WINDOW BUT ISSUING FROM THE DOOR SHE SAW MISS TILNEY HERSELF SHE WAS FOLLOWED BY A GENTLEMAN WHOM CATHERINE BELIEVED TO BE HER FATHER AND THEY TURNED UP TOWARDS EDGAR'S BUILDINGS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>THEIR MURDERER ALSO LIVED AND TO DESTROY HIM I MUST DRAG OUT MY WEARY EXISTENCE I KNELT ON THE GRASS AND KISSED THE EARTH AND WITH QUIVERING LIPS EXCLAIMED BY THE SACRED EARTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AS THE ART IS REAL SHE TOLD ME HOW POOR DEAR LADY BALDOCK COMMUNICATED TO YOU HER UNHAPPINESS ABOUT HER DAUGHTER IN A MANNER THAT MADE EVEN ME LAUGH AND WOULD MAKE THOUSANDS LAUGH IN DAYS TO COME WERE IT EVER TO BE PUBLISHED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SOMETIMES ON SUMMER AFTERNOONS SHE CAME OUT OF THE HOUSE AND GOT INTO HER CARRIAGE DISMISSING THE DRIVER SHE TOOK THE REINS IN HER OWN HANDS AND DROVE OFF AT TOP SPEED THROUGH THE STREETS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>THE OPENING VERY TIGHT FOR IT IS MANIFEST THAT THE TIE MODERATELY STRAIGHTENED WHILE ADEQUATE TO HINDER THE BLOOD ALREADY IN THE ARM FROM RETURNING TOWARDS THE HEART BY THE VEINS CANNOT ON THAT ACCOUNT PREVENT NEW BLOOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AND THAT I WAS RESERVED FOR VENGEANCE THE LAUGHTER DIED AWAY WHEN A WELL KNOWN AND ABHORRED VOICE APPARENTLY CLOSE TO MY EAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AND HANDSOME SHINING MATS OF THE KINNIKINIC SPRINKLED WITH BRIGHT SCARLET BERRIES FROM A PLACE CALLED HUNT'S AT THE END OF THE WAGON ROAD A TRAIL LEADS THROUGH LUSH DRIPPING WOODS NEVER DRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HAVING FOUND THE TRACES OF YESTERDAY'S STAG HE CREPT UNDER A BUSH INTO THE THICKET JUST WHERE THE STAG HAD LAIN AND LAY DOWN IN ITS LAIR HE EXAMINED THE DARK FOLIAGE AROUND HIM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AND WILL BE PLEASED TO STEP TO MY HOUSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>WE WERE NOT REALLY WHERE THE PROFESSOR SUPPOSED WE WERE IN FACT WE WERE NOT UPON THE NORTH SHORE OF THE SEA NOW LET US START UPON FRESH DISCOVERIES I SAID AND LEAVING HANS TO HIS WORK WE STARTED OFF TOGETHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>THAT NOW IS SOMETHING LIKE AND BEN WHO HAD PRICKED UP HIS EARS AT THE WORD CIRCUS LAID HIS FINGER ON A SMALLER CUT OF A MAN HANGING BY THE BACK OF HIS NECK WITH A CHILD IN EACH HAND TWO MEN SUSPENDED FROM HIS FEET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I HAVE GREAT RELIANCE HOWEVER ON THAT EXTREME DELICACY OF DISCRIMINATION IN MATTERS APPERTAINING TO THE RULES OF ETIQUETTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I SAY THE SECOND BEST SO THAT IF YOU REMIND ME OF TOM JONES OR THE MAYOR OF CASTERBRIDGE OR ANY OTHER THAT YOU FANCY I CAN SAY THAT OF COURSE THAT ONE IS THE BEST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>IN A SWAMPY PLACE COMING ALONG SANCH SAW SOMETHING DOWN THERE AND I WENT WITH HIM CAUSE I THOUGHT MAY BE IT WAS A MUSK RAT AND YOU'D LIKE ONE IF WE COULD GET HIM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>EIGHTEEN SEVENTY SIX I WILL GIVE HERE AS MUCH OF IT AS CONCERNS THE PUBLIC I WISH YOU TO ACCEPT AS A GIFT FROM ME GIVEN YOU NOW THE ACCOMPANYING PAGES WHICH CONTAIN A MEMOIR OF MY LIFE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>A PYRAMID SHAPED BLOCK OF SHALE WITH A VERY SHARP SUMMIT WHICH DEPENDING ON WHETHER IT'S CLEAR OR VEILED IN VAPOR PREDICTS FAIR WEATHER OR FOUL AS NED LAND TOLD ME A FIRST CLASS BAROMETER MY FRIEND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>TIPPED OVER AND DISAPPEARED INTO A CLUMP OF LILAC BUSHES IT WAS A VERY SHORT DISTANCE FORTUNATELY AND THE AMUSED CAPITALIST PICKED HER UP SET HER ON HER FEET AND BRUSHED HER OFF YOU SHOULD NEVER SEEM SURPRISED WHEN YOU HAVE TAKEN A LARGE ORDER SAID HE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(raw_datasets[\"train\"].remove_columns([\"file\", 'audio']), num_examples=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"]'\n",
    "\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower()\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = raw_datasets.map(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what do you mean shall i tell you certainly well then i guess that mister willoughby hunts marianne was surprised and confused yet she could not help smiling at the quiet archness of his manner and after a moment's silence said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>within that region where the sweet west wind rises to open the new leaves wherewith europe is seen to clothe herself afresh not far off from the beating of the waves behind which in his long career the sun sometimes conceals himself from every man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>yet they went there regularly of their own accord though they betrayed no greater delight in the experience than ourselves on the whole the existence of these olympians seemed to be entirely void of interests</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>that their curiosity was now on the point of being gratified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no replied uncle john the merricks are out of elmhurst now and it returns to its rightful owners you owe me nothing my lad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>indulging in faint and distant hopes of cherishing that life that i now abhor but while i was in the city uncertain what to do as i could not find don fernando i heard notice given by the public crier offering a great reward to anyone who should find me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i was extremely perplexed i will permit myself the liberty of saying i would fain believe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>any child could see that his eyes were tired and his mouth was sad when he was not speaking i'm doing pretty well thank you said the man with a delightful smile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>asked sellen colouring up to the roots of his hair i want it and a pair of fire tongs sellen gave him the required articles took his sketching stool and sat down on the pieces of cardboard as if he were guarding a treasure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the latter was silent brooding over the confidence which philip had apparently received but which was withheld from him he did not yet know of the culminating point of philip's proposed journey to london</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(raw_datasets[\"train\"].remove_columns([\"file\", \"audio\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    SchedulerType,\n",
    "    Wav2Vec2Config,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2ForPreTraining,\n",
    "    get_scheduler,\n",
    "    is_wandb_available,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import _compute_mask_indices, _sample_negative_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path=\"patrickvonplaten/wav2vec2-base-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor.sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor.do_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "# make sure that dataset decodes audio with correct sampling rate\n",
    "raw_datasets = raw_datasets.cast_column(\n",
    "    \"audio\", datasets.features.Audio(sampling_rate=feature_extractor.sampling_rate)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set max & min audio length in number of samples\n",
    "max_duration_in_seconds = 5 \n",
    "min_duration_in_seconds = 3   \n",
    "max_length = int(max_duration_in_seconds * feature_extractor.sampling_rate) #80000\n",
    "min_length = int(min_duration_in_seconds * feature_extractor.sampling_rate) #48000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    sample = batch[\"audio\"]\n",
    "\n",
    "    inputs = feature_extractor(\n",
    "        sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], max_length=max_length, truncation=True\n",
    "    )\n",
    "    batch[\"input_values\"] = inputs.input_values[0]\n",
    "    batch[\"input_length\"] = len(inputs.input_values[0])\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 28254/28254 [02:31<00:00, 186.79 examples/s]\n",
      "Map: 100%|██████████| 285/285 [00:02<00:00, 135.75 examples/s]\n"
     ]
    }
   ],
   "source": [
    "cache_file_names = None\n",
    "vectorized_datasets = raw_datasets.map(\n",
    "        prepare_dataset,\n",
    "        num_proc=1,\n",
    "        remove_columns=raw_datasets[\"train\"].column_names,\n",
    "        cache_file_names=cache_file_names,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 28254/28254 [00:00<00:00, 504548.65 examples/s]\n",
      "Filter: 100%|██████████| 285/285 [00:00<00:00, 31635.44 examples/s]\n"
     ]
    }
   ],
   "source": [
    "if min_length > 0.0:\n",
    "    vectorized_datasets = vectorized_datasets.filter(\n",
    "        lambda x: x > min_length,\n",
    "        num_proc=1,\n",
    "        input_columns=[\"input_length\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_datasets = vectorized_datasets.remove_columns(\"input_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_values'],\n",
       "        num_rows: 27667\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_values'],\n",
       "        num_rows: 282\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load model\n",
    "config = Wav2Vec2Config.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretraining is only supported for \"newer\" stable layer norm architecture\n",
    "# apply_spec_augment has to be True, mask_feature_prob has to be 0.0\n",
    "if not config.do_stable_layer_norm or config.feat_extract_norm != \"layer\":\n",
    "    raise ValueError(\n",
    "        \"PreTraining is only supported for ``config.do_stable_layer_norm=True`` and\"\n",
    "        \" ``config.feat_extract_norm='layer'\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize random model\n",
    "model = Wav2Vec2ForPreTraining(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForWav2Vec2Pretraining:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received and prepare masked indices\n",
    "    for self-supervised pretraining.\n",
    "\n",
    "    Args:\n",
    "        model (:class:`~transformers.Wav2Vec2ForPreTraining`):\n",
    "            The Wav2Vec2 model used for pretraining. The data collator needs to have access\n",
    "            to config and ``_get_feat_extract_output_lengths`` function for correct padding.\n",
    "        feature_extractor (:class:`~transformers.Wav2Vec2FeatureExtractor`):\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "        mask_time_prob (:obj:`float`, `optional`, defaults to :obj:`0.65`):\n",
    "            Percentage (between 0 and 1) of all feature vectors along the time axis which will be masked for the contrastive task.\n",
    "            Note that overlap between masked sequences may decrease the actual percentage of masked vectors.\n",
    "            The default value is taken from the original wav2vec 2.0 article (https://arxiv.org/abs/2006.11477),\n",
    "            and results in about 49 percent of each sequence being masked on average.\n",
    "        mask_time_length (:obj:`int`, `optional`, defaults to :obj:`10`):\n",
    "            Length of each vector mask span to mask along the time axis in the contrastive task. The default value\n",
    "            originates from the original wav2vec 2.0 article and corresponds to the ``M`` variable mentioned there.\n",
    "    \"\"\"\n",
    "\n",
    "    model: Wav2Vec2ForPreTraining\n",
    "    feature_extractor: Wav2Vec2FeatureExtractor\n",
    "    padding: Union[bool, str] = \"longest\"\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    mask_time_prob: Optional[float] = 0.65\n",
    "    mask_time_length: Optional[int] = 10\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # reformat list to dict and set to pytorch format\n",
    "        batch = self.feature_extractor.pad(\n",
    "            features,\n",
    "            padding=self.padding,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        device = batch[\"input_values\"].device\n",
    "        batch_size = batch[\"input_values\"].shape[0]\n",
    "\n",
    "        mask_indices_seq_length = self.model._get_feat_extract_output_lengths(batch[\"input_values\"].shape[-1])\n",
    "        # make sure masked sequence length is a Python scalar\n",
    "        mask_indices_seq_length = int(mask_indices_seq_length)\n",
    "\n",
    "        # make sure that no loss is computed on padded inputs\n",
    "        if batch.get(\"attention_mask\") is not None:\n",
    "            # compute real output lengths according to convolution formula\n",
    "            batch[\"sub_attention_mask\"] = self.model._get_feature_vector_attention_mask(\n",
    "                mask_indices_seq_length, batch[\"attention_mask\"]\n",
    "            )\n",
    "\n",
    "        features_shape = (batch_size, mask_indices_seq_length)\n",
    "\n",
    "        # sample randomly masked indices\n",
    "        mask_time_indices = _compute_mask_indices(\n",
    "            features_shape,\n",
    "            self.mask_time_prob,\n",
    "            self.mask_time_length,\n",
    "            attention_mask=batch.get(\"sub_attention_mask\"),\n",
    "        )\n",
    "\n",
    "        # sample negative indices\n",
    "        sampled_negative_indices = _sample_negative_indices(\n",
    "            features_shape,\n",
    "            self.model.config.num_negatives,\n",
    "            mask_time_indices=mask_time_indices,\n",
    "        )\n",
    "        batch[\"mask_time_indices\"] = torch.tensor(mask_time_indices, dtype=torch.long, device=device)\n",
    "        batch[\"sampled_negative_indices\"] = torch.tensor(sampled_negative_indices, dtype=torch.long, device=device)\n",
    "\n",
    "        return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForWav2Vec2Pretraining(\n",
    "        model=model,\n",
    "        feature_extractor=feature_extractor,\n",
    "        pad_to_multiple_of=None,\n",
    "        mask_time_prob=config.mask_time_prob,\n",
    "        mask_time_length=config.mask_time_length,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting jiwer\n",
      "  Downloading jiwer-3.0.3-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in c:\\users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages (from jiwer) (8.1.7)\n",
      "Collecting rapidfuzz<4,>=3 (from jiwer)\n",
      "  Downloading rapidfuzz-3.6.1-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages (from click<9.0.0,>=8.1.3->jiwer) (0.4.6)\n",
      "Downloading jiwer-3.0.3-py3-none-any.whl (21 kB)\n",
      "Downloading rapidfuzz-3.6.1-cp310-cp310-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.6 MB 8.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.8/1.6 MB 10.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.6/1.6 MB 12.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 13.0 MB/s eta 0:00:00\n",
      "Installing collected packages: rapidfuzz, jiwer\n",
      "Successfully installed jiwer-3.0.3 rapidfuzz-3.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = feature_extractor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = feature_extractor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = feature_extractor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:1629: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.Please use the equivalent `freeze_feature_encoder` method instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./output/wav2vec2-base-asr-demo\",\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=32,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=30,\n",
    "  fp16=True,\n",
    "  save_steps=500,\n",
    "  eval_steps=500,\n",
    "  logging_steps=500,\n",
    "  learning_rate=1e-4,\n",
    "  weight_decay=0.005,\n",
    "  warmup_steps=1000,\n",
    "  save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=vectorized_datasets[\"train\"],\n",
    "    eval_dataset=vectorized_datasets[\"validation\"],\n",
    "    tokenizer=feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(\n",
    "    vectorized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    vectorized_datasets[\"validation\"], collate_fn=data_collator, batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lkk68\\.conda\\envs\\mycondapy310\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Optimizer\n",
    "learning_rate=5e-5\n",
    "adam_beta1=0.9\n",
    "adam_beta2=0.999\n",
    "adam_epsilon=1e-8\n",
    "optimizer = AdamW(\n",
    "    list(model.parameters()),\n",
    "    lr=learning_rate,\n",
    "    betas=[adam_beta1, adam_beta2],\n",
    "    eps=adam_epsilon,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_accumulation_steps =1\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3459"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_update_steps_per_epoch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mycondapy310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
